spring:
  kafka:
    consumer:
      auto-offset-reset: latest
      group-id: group_id
  sleuth:
    otel:
      config:
        trace-id-ratio-based: 1.0
      exporter:
        otlp:
          endpoint: http://localhost:4317
  application:
    name: boot3
  otel:
    exporter:
      otlp:
        endpoint: http://localhost:4317
  cloud:
    stream:
      bindings:
        input-channel:
          destination: validate
        output-channel:
          destination: messageprocessed
      kafka:
        streams:
          binder:
            deserialization-exception-handler: sendToDlq
            applicationId: validateApp
            brokers: localhost:9092
            configuration:
              processing.guarantee: exactly_once_v2
              default:
                key:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
                value:
                  serde: org.apache.kafka.common.serialization.Serdes$StringSerde
          bindings:
            output-channel:
              producer:
                valueSerde: org.apache.kafka.common.serialization.Serdes$StringSerde
              consumer:
                dlqName: topic-error
# traceID and spanId are predefined MDC keys - we want the logs to include them
logging.pattern.level: "%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]"
management:
  tracing:
    sampling:
      probability: 1.0
  endpoints:
    web:
      exposure:
        include: "*"
    enabled-by-default: true
otel:
  exporter:
    otlp:
      endpoint: http://localhost:4317
kafka:
  consumer:
    topic: input
    properties:
      auto.offset.reset: earliest
      bootstrap.servers: sourcekafka-1.example.com:9092
      enable.auto.commit: false
      group.id: transactional.group.abc
      isolation.level: read_committed
      key.deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
      max.poll.records: 1000
      value.deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
      max.poll.interval.ms: 120000
  producer:
    topic: output
    offset-topic: __targetOffsetTopic
    commit-batch-size: 1000
    commit-max-interval-ms: 5000
    transaction-id-prefix: producer1.
    properties:
      batch.size: 262144
      bootstrap.servers: targetkafka-1.example.com:9092
      buffer.memory: 33554432
      compression.type: snappy
      enable.idempotence: true
      key.serializer: org.apache.kafka.common.serialization.ByteArraySerializer
      value.serializer: org.apache.kafka.common.serialization.ByteArraySerializer
      linger.ms: 200
      max.in.flight.requests.per.connection: 4
      request.timeout.ms: 60000
      transaction.timeout.ms: 120000